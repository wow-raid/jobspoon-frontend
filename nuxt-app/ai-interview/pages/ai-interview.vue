<template>
  <v-container v-if="!start" align="center">
    <div class="interview-container">
      <v-icon>mdi-account-tie</v-icon><br />
      <div v-html="startMessage"></div>

      <!-- âœ… ì¹´ë©”ë¼+ë§ˆì´í¬ ë…¹í™” í™•ì¸ UI -->
      <div style="margin-top: 16px">
        <video
          ref="previewVideo"
          autoplay
          playsinline
          muted
          style="
            width: 250px;
            height: 188px;
            background: #000;
            border-radius: 8px;
          "
        />
        <div class="button-group">
          <v-btn color="info" @click="checkMediaReady"
            >ì¹´ë©”ë¼/ë§ˆì´í¬ ìƒíƒœ í™•ì¸</v-btn
          >
          <v-btn color="success" @click="startRecording">ğŸ¥ ë…¹í™” ì‹œì‘</v-btn>
          <v-btn color="error" @click="stopRecording">ğŸ›‘ ë…¹í™” ì¢…ë£Œ</v-btn>
          <v-btn
            color="warning"
            @click="playRecording"
            :disabled="!recordedBlob"
            >â–¶ ì˜ìƒ ì¬ìƒ</v-btn
          >
        </div>
      </div>

      <!-- ë©´ì ‘ ì‹œì‘ ë²„íŠ¼ -->
      <v-btn
        color="primary"
        class="mt-4"
        @click="handleStartInterview"
        :disabled="!mediaChecked"
      >
        ë©´ì ‘ ì‹œì‘
      </v-btn>
    </div>
  </v-container>

  <!-- ì´í•˜ ê¸°ì¡´ start=true ìƒíƒœ -->
  <v-container v-else fluid class="pa-0">
    <div style="width: 75%; margin: 0 auto; padding-top: 16px">
      <v-row class="video-row" no-gutters style="margin: 0; padding: 0">
        <v-col
          cols="6"
          class="pa-0"
          style="display: flex; justify-content: flex-end"
        >
          <div class="video-box" style="width: 100%; height: 300px">
            <img :src="hhImage" alt="ë©´ì ‘ê´€" class="interviewer-image" />
          </div>
        </v-col>

        <v-col class="pa-0" style="max-width: 16px"></v-col>

        <v-col
          cols="6"
          class="pa-0"
          style="display: flex; justify-content: flex-start"
        >
          <div class="video-box" style="width: 100%; height: 300px">
            <video
              ref="userVideo"
              v-show="start"
              autoplay
              playsinline
              muted
              class="user-video"
            ></video>
          </div>
        </v-col>
      </v-row>
    </div>

    <v-col class="pa-0" style="max-width: 16px"></v-col>

    <v-col
      cols="12"
      class="pa-0 mt-4"
      style="display: flex; justify-content: center"
    >
      <div
        v-if="visible"
        class="interview-container centered-text-box"
        style="margin-top: 0; width: 75%"
      >
        <v-icon>mdi-account-tie</v-icon><br />
        <div v-html="startMessage"></div>
      </div>
      <div
        v-else
        class="interview-container centered-text-box"
        style="margin-top: 0; width: 75%"
      >
        <v-icon>mdi-account-tie</v-icon><br />
        <h2 v-html="formattedAIMessage"></h2>
        <br />
        <div :class="{ timer: true, 'red-text': remainingTime <= 10 }">
          ë‚¨ì€ ì‹œê°„: {{ Math.floor(remainingTime / 60) }}:{{
            (remainingTime % 60).toString().padStart(2, "0")
          }}
        </div>
      </div>
    </v-col>

    <div v-if="isLoading && !finished" class="message ai">
      <br />
      <p><strong>ë‹¤ìŒ ì§ˆë¬¸ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.</strong></p>
      <v-icon>mdi-account-tie</v-icon>
      <div class="loading-message">
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
      </div>
    </div>

    <v-container v-if="start && !visible" class="input-area">
      <!-- ë²„íŠ¼ ê·¸ë£¹ -->
      <div class="button-group">
        <button class="send-button" @click="startSTT" :disabled="recognizing">
          {{ recognizing ? "ë…¹ìŒ ì¤‘..." : "ë§í•˜ê¸°" }}
        </button>
        <button @click="replayQuestion">ğŸ—£ AI ì§ˆë¬¸ ë“£ê¸°</button>
      </div>

      <!-- ë‹µë³€ ì™„ë£Œ ë²„íŠ¼ -->
      <v-btn color="primary" @click="onAnswerComplete" :disabled="isGenerating">
        <template v-if="isGenerating && finished.value">
          ê²°ê³¼ ìƒì„± ì¤‘...
        </template>
        <template v-else-if="isGenerating"> ì§ˆë¬¸ ìƒì„± ì¤‘... </template>
        <template v-else> ë‹µë³€ ì™„ë£Œ </template>
      </v-btn>

      <!-- STT ìƒíƒœ ë©”ì‹œì§€ / ê²°ê³¼ -->
      <div class="stt-log text-center" style="margin-top: 8px">
        <!-- ğŸ™ï¸ STT ì…ë ¥ ì¤‘ -->
        <template v-if="recognizing">
          <p>ğŸ™ï¸ ì…ë ¥ ì¤‘ì…ë‹ˆë‹¤...</p>
        </template>

        <!-- ğŸ¤– ì§ˆë¬¸ ìƒì„± ì¤‘ or ğŸ“ ê²°ê³¼ ìƒì„± ì¤‘ + STT ê²°ê³¼ ê°™ì´ ë³´ì—¬ì¤Œ -->
        <template v-else-if="isGenerating">
          <p style="color: gray">
            {{
              finished.value
                ? "ğŸ“ ê²°ê³¼ë¥¼ ìƒì„±í•˜ê³  ìˆì–´ìš”..."
                : "ğŸ¤– ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³  ìˆì–´ìš”..."
            }}
          </p>

          <p v-if="sttLog !== ''"><strong>STT ê²°ê³¼:</strong> {{ sttLog }}</p>
        </template>

        <!-- âœ… STT ê²°ê³¼ë§Œ ìˆì„ ë•Œ -->
        <template v-else-if="sttLog !== ''">
          <p><strong>STT ê²°ê³¼:</strong> {{ sttLog }}</p>
        </template>
      </div>

      <!-- ì§ì ‘ ì…ë ¥ í•„ë“œ -->
      <v-text-field
        v-model="sttLog"
        label="ê°œë°œ ì¤‘: ë‹µë³€ ì§ì ‘ ì…ë ¥"
        hide-details
        dense
        solo
        style="max-width: 300px"
      />
    </v-container>
  </v-container>
</template>

<script setup>
import { ref, computed, onMounted, onBeforeUnmount, nextTick } from "vue";
import { useAiInterviewStore } from "../stores/aiInterviewStore";
import { useRouter, onBeforeRouteLeave } from "vue-router";
import "@mdi/font/css/materialdesignicons.css";
import hhImage from "@/assets/images/fixed/al3.png";

// âœ… SEO ë©”íƒ€ ì •ë³´
definePageMeta({
  title: "AI ëª¨ì˜ ë©´ì ‘ ì‹œì‘ | ì¡ìŠ¤í‹±(JobStick)",
  description:
    "AI ê¸°ë°˜ ëª¨ì˜ ë©´ì ‘ì„ ì§„í–‰í•˜ê³ , ì›í•˜ëŠ” ê¸°ì—…, ì§ë¬´ì— ëŒ€í•œ ê¸°ìˆ  ë©´ì ‘ì„ ëŒ€ë¹„í•´ë³´ì„¸ìš”.",
  keywords: [
    "AI ë©´ì ‘",
    "ëª¨ì˜ ë©´ì ‘",
    "ê¸°ìˆ  ë©´ì ‘",
    "AI ê¸°ë°˜ ëª¨ì˜ ë©´ì ‘",
    "ì¹´ë©”ë¼ í…ŒìŠ¤íŠ¸",
    "ë©´ì ‘ ì¤€ë¹„",
    "ì˜¨ë¼ì¸ ë©´ì ‘",
    "ë¹„ëŒ€ë©´ ë©´ì ‘",
    "ì¸ê³µì§€ëŠ¥ ë©´ì ‘",
    "JobStick",
    "job-stick",
    "ì¡ìŠ¤í‹±",
    "ê°œë°œì í”Œë«í¼",
    "ê°œë°œì ì·¨ì—…",
  ],
  ogTItle: "AI ëª¨ì˜ ë©´ì ‘ ì¤€ë¹„ - ì¡ìŠ¤í‹±(JobStick)",
  ogDescription:
    "ì¡ìŠ¤í‹±(JobStick)ì—ì„œ AI ê¸°ë°˜ì˜ ì‹¤ì „ ë©´ìŠµ ì—°ìŠµì„ í†µí•´ ê¸°ìˆ  ë©´ì ‘ì˜ ì‹¤ì „ ê°ê°ì„ ê¸¸ëŸ¬ë³´ì„¸ìš”.",
  ogImage: "", // ì‹¤ì œ ì´ë¯¸ì§€ ê²½ë¡œ
  robots: "index, follow", // ê²€ìƒ‰ ë…¸ì¶œ í—ˆìš©
});

const router = useRouter();
const aiInterviewStore = useAiInterviewStore();

const start = ref(false);
const visible = ref(true);
const isLoading = ref(false);
const finished = ref(false);
const recognizing = ref(false);
const sttLog = ref("");
const currentAIMessage = ref("");
const currentQuestionId = ref(1);
const currentInterviewId = ref(null);
const remainingTime = ref(90);
const timer = ref(null);
const maxQuestionId = ref(10);
const startMessage = ref("");
const userVideo = ref(null);
const mediaChecked = ref(false);
const previewVideo = ref(null);
const mediaStream = ref(null);
const isGenerating = ref(false); // ì§ˆë¬¸ ìƒì„± ì¤‘ ì—¬ë¶€

const mapCompanyName = (original) => {
  const mapping = {
    ë‹¹ê·¼ë§ˆì¼“: "danggeun",
    Toss: "toss",
    "SK-encore": "sk_encore",
    "KT M mobile": "kt_mobile",
  };
  return mapping[original] || original.toLowerCase().replace(/[\s-]+/g, "_");
};

const checkMediaReady = async () => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({
      video: true,
      audio: true,
    });
    mediaChecked.value = true;
    alert("ë§ˆì´í¬ì™€ ì¹´ë©”ë¼ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.");
    mediaStream.value = stream;
    if (previewVideo.value) previewVideo.value.srcObject = stream;
  } catch (err) {
    alert(
      "ë§ˆì´í¬ ë˜ëŠ” ì¹´ë©”ë¼ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì € ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”."
    );
    mediaChecked.value = false;
  }
};

const recordedVideo = ref(null); // ìƒë‹¨ì— ì„ ì–¸ í•„ìš”
const recordedBlob = ref(null); // ìƒë‹¨ì— ì„ ì–¸ í•„ìš”
let recordingStream = null;
let recorder = null;
let chunks = [];
//ë©´ì ‘ ì „ ë…¹í™” ë˜ëŠ”ì§€ í™•ì¸ìš©
const startRecording = async () => {
  try {
    recordingStream = await navigator.mediaDevices.getUserMedia({
      video: true,
      audio: true,
    });
    if (previewVideo.value) previewVideo.value.srcObject = recordingStream;

    chunks = [];
    recorder = new MediaRecorder(recordingStream, { mimeType: "video/webm" });

    recorder.ondataavailable = (event) => {
      if (event.data.size > 0) {
        chunks.push(event.data);
      }
    };

    recorder.onstop = () => {
      recordedBlob.value = new Blob(chunks, { type: "video/webm" });
      const videoURL = URL.createObjectURL(recordedBlob.value);
      localStorage.setItem("interviewRecordingUrl", videoURL); //ë¡œì»¬ì— ì„ì‹œì €ì¥
      // âœ… previewVideoì—ì„œ ë…¹í™” ì˜ìƒ ì¬ìƒ
      if (previewVideo.value) {
        previewVideo.value.srcObject = null; // ìŠ¤íŠ¸ë¦¼ ëŠê¸°
        previewVideo.value.src = videoURL;
        previewVideo.value.controls = true;
        previewVideo.value.play();
      }
    };
    //ì˜ìƒë…¹í™”
    recorder.start();
    alert("ë…¹í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ë‹¤");
  } catch (err) {
    console.error("ğŸ¥ ë…¹í™” ì‹œì‘ ì‹¤íŒ¨:", err);
    alert("ë…¹í™” ì‹œì‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.");
  }
};
//ë…¹í™”ì¤‘ì§€
const stopRecording = () => {
  if (recorder && recorder.state === "recording") {
    recorder.stop();
    if (recordingStream) {
      recordingStream.getTracks().forEach((track) => track.stop());
    }
    alert("ë…¹í™” ì¢…ë£Œë¨");
  }
};

let recognition;
const synth = process.client ? window.speechSynthesis : null;
let currentUtteance = null;

onMounted(async () => {
  try {
    const videoOnlyStream = await navigator.mediaDevices.getUserMedia({
      video: true,
    });
    if (previewVideo.value) {
      previewVideo.value.srcObject = videoOnlyStream;
    }
  } catch (err) {
    console.error("previewVideo ì¹´ë©”ë¼ ì—°ê²° ì‹¤íŒ¨:", err);
  }
  if (process.client) {
    speakStartMessage();

    await nextTick(); // âœ… DOM ì¤€ë¹„ëœ ë‹¤ìŒ

    const SpeechRecognition =
      window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) {
      alert("ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.");
      return;
    }

    recognition = new SpeechRecognition();
    recognition.lang = "ko-KR";
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.onstart = () => (recognizing.value = true);
    recognition.onend = () => (recognizing.value = false);
    recognition.onerror = () => (recognizing.value = false);
    recognition.onresult = (event) => {
      let finalTranscript = "";
      for (let i = event.resultIndex; i < event.results.length; ++i) {
        if (event.results[i].isFinal) {
          finalTranscript += event.results[i][0].transcript;
        }
      }
      sttLog.value += finalTranscript;
    };

    window.addEventListener("beforeunload", handleBeforeUnload);
  }
});

const showWarning = ref(true); // ì´ê±° ë¨¼ì € ì„ ì–¸ë¼ ìˆì–´ì•¼ í•¨

const speakStartMessage = () => {
  startMessage.value = `
    <strong style="display: flex; flex-direction: column; align-items: center;">
      <span style="margin-bottom: 8px;">AI ëª¨ì˜ ë©´ì ‘ì´ ê³§ ì‹œì‘ë©ë‹ˆë‹¤.</span>
      <span style="margin-bottom: 8px;">ë©´ì ‘ ì§ˆë¬¸ì´ í™”ë©´ì— í‘œì‹œë˜ë©°, ìë™ìœ¼ë¡œ ìŒì„±ìœ¼ë¡œ ì½ì–´ë“œë¦½ë‹ˆë‹¤.</span>
      <span style="margin-bottom: 8px;"><mark style="background: #ffecb3;">ë§í•˜ê¸° ë²„íŠ¼</mark>ì„ ëˆŒëŸ¬ ë‹µë³€ì„ ì‹œì‘í•´ ì£¼ì„¸ìš”.</span>
      <span style="margin-bottom: 8px;">ë§ˆì´í¬ì™€ ì¹´ë©”ë¼ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì¸ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.</span>
      ${
        showWarning.value
          ? '<span style="color: red; font-weight: bold;">â€» ì¹´ë©”ë¼/ë§ˆì´í¬ ìƒíƒœ í™•ì¸ì„ ì²´í¬í•´ì•¼ ë©´ì ‘ ì‹œì‘ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.</span>'
          : ""
      }
    </strong>
  `;
};

const formattedAIMessage = computed(() => {
  return currentAIMessage.value.replace(/([.?])/g, "$1<br>");
});

const replayQuestion = () => {
  if (synth?.speaking) synth.cancel();
  const utterance = new SpeechSynthesisUtterance(currentAIMessage.value);
  utterance.lang = "ko-KR";
  utterance.rate = 0.85;
  utterance.pitch = 1.0;
  setTimeout(() => synth?.speak(utterance), 100);
};

const handleBeforeUnload = (event) => {
  if (start.value && !finished.value) {
    event.preventDefault();
    event.returnValue = "ë©´ì ‘ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ë‚˜ê°€ì‹œê² ìŠµë‹ˆê¹Œ?";
  }
};

const speakCurrentMessage = () => {
  clearInterval(timer.value);
  remainingTime.value = 90;
  currentUtteance = new SpeechSynthesisUtterance(currentAIMessage.value);
  currentUtteance.lang = "ko-KR";
  currentUtteance.rate = 0.85;
  currentUtteance.pitch = 1.0;
  currentUtteance.onend = () => startTimer();
  synth?.speak(currentUtteance);
};

const showStartMessage = () => {
  visible.value = false;
  speakCurrentMessage();
};

const startTimer = () => {
  clearInterval(timer.value);
  timer.value = setInterval(() => {
    if (remainingTime.value > 0) {
      remainingTime.value--;
    } else {
      clearInterval(timer.value);
      onAnswerComplete();
    }
  }, 1000);
};

const startSTT = () => {
  if (recognition && !recognizing.value) {
    sttLog.value = "";
    recognition.start();
  }
};
//ìë™ ë…¹í™” ì‹œì‘
const startRecordingAuto = async () => {
  try {
    recordingStream = await navigator.mediaDevices.getUserMedia({
      video: true,
      audio: true,
    });
    if (userVideo.value) {
      userVideo.value.srcObject = recordingStream;
    }
    chunks = [];
    recorder = new MediaRecorder(recordingStream, { mimeType: "video/webm" });
    recorder.ondataavailable = (event) => {
      if (event.data.size > 0) chunks.push(event.data);
    };
    recorder.onstop = () => {
      recordedBlob.value = new Blob(chunks, { type: "video/webm" });
      const videoURL = URL.createObjectURL(recordedBlob.value);
      localStorage.setItem("interviewRecordingUrl", videoURL);
    };
    recorder.start();
    console.log("ë…¹í™” ì‹œì‘");
  } catch (err) {
    console.error("ë…¹í™” ì‹¤íŒ¨");
  }
};
// ìë™ ë…¹í™” ì¢…ë£Œë£Œ
const stopRecordingAuto = () => {
  if (recorder && recorder.state === "recording") {
    recorder.stop();
    if (recordingStream) {
      recordingStream.getTracks().forEach((track) => track.stop());
    }
    console.log("ë…¹í™” ì¢…ë£Œ");
  }
};
const handleStartInterview = async () => {
  const info = JSON.parse(localStorage.getItem("interviewInfo") || "{}");
  const processedCompanyName = mapCompanyName(info.company);

  if (!info.tech || !info.exp) {
    alert("ë©´ì ‘ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.");
    router.push("/ai-interview");
    return;
  }
  start.value = true;
  await startRecordingAuto(); //ë…¹í™” ì‹œì‘

  showWarning.value = false;
  speakStartMessage();

  const res = await aiInterviewStore.requestCreateInterviewToDjango({
    userToken: localStorage.getItem("userToken"),
    jobCategory: info.tech,
    experienceLevel: info.exp,
    academicBackground: info.academic,
    projectExperience: info.project,
    interviewTechStack: info.skills,
    companyName: processedCompanyName,
  });
  currentInterviewId.value = Number(res.interviewId);
  currentQuestionId.value = 1;
  currentAIMessage.value = res.question;
  const utterance = new SpeechSynthesisUtterance(
    "AI ëª¨ì˜ ë©´ì ‘ì´ ê³§ ì‹œì‘ë©ë‹ˆë‹¤. ë©´ì ‘ ì§ˆë¬¸ì´ í™”ë©´ì— í‘œì‹œë˜ë©°, ìë™ìœ¼ë¡œ ìŒì„±ìœ¼ë¡œ ì½ì–´ë“œë¦½ë‹ˆë‹¤."
  );
  utterance.lang = "ko-KR";
  utterance.rate = 1;
  utterance.pitch = 1;
  utterance.onend = () => showStartMessage();
  synth?.cancel();
  synth?.speak(utterance);
};

const onAnswerComplete = async () => {
  isGenerating.value = true; // âœ… ì§ˆë¬¸ ìƒì„± ì‹œì‘

  clearInterval(timer.value);
  if (recognition && recognizing.value) recognition.stop();

  if (!sttLog.value.trim()) {
    alert("ìŒì„± ì¸ì‹ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.");
    isGenerating.value = false;
    return;
  }
  if (currentQuestionId.value >= maxQuestionId.value) {
    alert("ëª¨ë“  ë©´ì ‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤");
    finished.value = true;
    isGenerating.value = true; // âœ… ê²°ê³¼ ìƒì„± ì¤‘ ìƒíƒœ ìœ ì§€ (ì¢…ë£Œë˜ë©´ router ì´ë™)
    return;
  }

  const info = JSON.parse(localStorage.getItem("interviewInfo") || "{}");
  const processedCompanyName = mapCompanyName(info.company);
  const payload = {
    userToken: localStorage.getItem("userToken"),
    interviewId: currentInterviewId.value,
    questionId: currentQuestionId.value,
    answerText: sttLog.value,
    jobCategory: info.tech,
    experienceLevel: info.exp,
    academicBackground: info.academic,
    projectExperience: info.project,
    interviewTechStack: info.skills,
    companyName: processedCompanyName,
  };
  await aiInterviewStore.requestCreateAnswerToDjango(payload);

  let nextQuestion = null;
  let nextQuestionId = null;
  if (currentQuestionId.value === 1) {
    const followUp = await aiInterviewStore.requestFollowUpQuestionToDjango(
      payload
    );
    nextQuestion = followUp?.questions?.[0];
    nextQuestionId = followUp?.questionIds?.[0];
  } else if (currentQuestionId.value === 2) {
    const projectMain =
      await aiInterviewStore.requestProjectCreateInterviewToDjango(payload);
    nextQuestion = projectMain?.question?.[0];
    nextQuestionId = projectMain?.questionId;
  } else if (currentQuestionId.value === 3 || currentQuestionId.value === 4) {
    const projectFollowUp =
      await aiInterviewStore.requestProjectFollowUpQuestionToDjango(payload);
    nextQuestion = projectFollowUp?.questions?.[0];
    nextQuestionId = projectFollowUp?.questionIds?.[0];
  } else if (currentQuestionId.value === 5) {
    //|| currentQuestionId.value === 6
    const techFollowUp =
      await aiInterviewStore.requestTechFollowUpQuestionToDjango(payload);
    nextQuestion = techFollowUp?.questions?.[0];
    nextQuestionId = techFollowUp?.questionIds?.[0];
  } else {
    finished.value = true; // 1. ë©´ì ‘ ì¢…ë£Œ ìƒíƒœ
    await nextTick();
    isGenerating.value = true; // 2. ê²°ê³¼ ìƒì„± ì¤‘ìœ¼ë¡œ ìƒíƒœ ì „í™˜
    await nextTick(); // 3. ë©”ì‹œì§€ DOM ë°˜ì˜

    alert("ëª¨ë“  ë©´ì ‘ ì§ˆë¬¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."); // 4. íŒì—… ì´í›„ ê²°ê³¼ ì²˜ë¦¬

    clearInterval(timer.value);
    stopRecordingAuto();
    await aiInterviewStore.requestEndInterviewToDjango(payload);
    await aiInterviewStore.requestGetScoreResultListToDjango(payload);
    router.push("/ai-interview/result");
    return;
  }

  if (!nextQuestion || !nextQuestionId) {
    alert("ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.");
    isGenerating.value = false;
    return;
  }

  currentQuestionId.value = nextQuestionId;
  currentAIMessage.value = nextQuestion;
  sttLog.value = "";
  speakCurrentMessage();

  isGenerating.value = false; // âœ… ë¡œë”© ì¢…ë£Œ
};

onBeforeUnmount(() => {
  if (synth?.speaking) synth.cancel();
  localStorage.removeItem("interviewInfo");
  clearInterval(timer.value);
  window.removeEventListener("beforeunload", handleBeforeUnload);
});

onBeforeRouteLeave((to, from, next) => {
  if (start.value && !finished.value) {
    const answer = window.confirm(
      "ë©´ì ‘ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ë‚˜ê°€ì‹œê² ìŠµë‹ˆê¹Œ?"
    );
    answer ? next() : next(false);
  } else {
    next();
  }
});
const playRecording = () => {
  if (recordedBlob.value) {
    const videoURL = URL.createObjectURL(recordedBlob.value);
    if (previewVideo.value) {
      previewVideo.value.srcObject = null;
      previewVideo.value.src = videoURL;
      previewVideo.value.controls = true;
      previewVideo.value.play();
    }
  }
};
</script>

<style scoped>
.interview-container {
  margin-top: 32px;
  border: 1px solid #333;
  padding: 16px;
  border-radius: 10px;
  width: 70%;
  display: flex;
  flex-direction: column;
  align-items: center;
  text-align: center;
}
.button-group {
  display: flex;
  flex-wrap: wrap;
  justify-content: center;
  gap: 12px;
  margin-top: 12px;
}

@media (max-width: 768px) {
  .button-group {
    flex-direction: column;
    align-items: stretch;
  }

  .button-group > * {
    width: 100%;
  }
}
.input-area {
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 20px;
  width: 50%;
  margin-bottom: 0;
}

.send-button {
  padding: 10px 12px;
  background-color: black;
  color: white;
  border: none;
  border-radius: 20px;
  cursor: pointer;
  font-size: 16px;
  height: 40px;
  display: flex;
  align-items: center;
  justify-content: center;
  min-width: 100px;
  box-sizing: border-box;
}

.video-row {
  margin-top: 24px;
  margin-bottom: 24px;
}

.video-box {
  width: 100%;
  aspect-ratio: 4 / 3;
  border: 2px solid #ccc;
  border-radius: 12px;
  overflow: hidden;
  display: flex;
  justify-content: center;
  align-items: center;
  background-color: #000;
}

.interviewer-image,
.user-video {
  width: 100%;
  height: 100%;
  object-fit: cover;
}
</style>
